\section{Appendix}

\subsection{Appendix A: TF-IDF and Filtering Methods}
\label{sec:tfidf_methods}

As mentioned earlier, TF-IDF helped to identify areas of the research in applications of LLMs. It was applied at multiple stages: the full corpus ($51{,}613$ records after deduplication), the curated review set ($195$ articles), and filtered variants where generic AI/ML phrases were removed (to move beyond obvious LLM keywords such as "language model", "deep learning" -- full pattern list in Supplementary Methods).

Bigram-trigram TF-IDF scores were computed (scikit-learn's \texttt{TfidfVectorizer} with \texttt{ngram\_range=(2,3)} and \texttt{max\_features=1000}), lower-casing and removing English stop-words plus custom artifacts (e.g., "et al").

\subsubsection{Context-Preserving Fine-Tuned Analysis (Reviewer Comment \#2)}

In response to reviewer feedback, we implemented a context-preserving fine-tuning approach. This method first trains the TF-IDF model on the curated dataset ($195$ articles) with full terminology context, then applies post-hoc reweighting by zeroing out generic AI/ML anchor terms and renormalizing the document vectors. This preserves the semantic context during initial feature extraction while down-weighting generic phrases in the final ranking. The fine-tuned analysis confirms that the domain-specific trends reported in the main text (e.g., precision medicine, gene expression, genetic testing) remain stable once obvious anchors are de-emphasized, demonstrating that our findings are robust to different filtering strategies.

Sources within the curated set were compared by stratifying PubMed ($n=131$) versus preprints (bioRxiv/medRxiv/arXiv; $n=64$). For each comparison, the union of the two top-30 lists was applied. This helped to capture the shift from generic to domain-specific terminology and highlight complementary emphases between peer-reviewed and preprint venues.

Additional representations of TF-IDF analysis are shown in four supplementary figures: Supplementary Figure~\ref{SF1} (three-stage progression of TF-IDF after selecting articles and filtering words), Supplementary Figure~\ref{SF2} (comparison of sources before filtering), Supplementary Figure~\ref{SF3} (comparison of sources after filtering), and Supplementary Figure~\ref{SF5} (comparison of sources using fine-tuned analysis).

In addition to standard English stop-words, generic AI/ML phrases and artifacts were excluded to surface domain-specific terminology. The final list included the following common terms: large language, language model, llm, llms, generative ai, foundation model, foundation models, deep learning, deep neural, neural network, neural networks, machine learning, artificial intelligence, artificial neural, natural language, language processing, nlp, transformer model, transformer models, reinforcement learning, supervised learning, unsupervised learning, state art, based, using, https, github, model, models, learning, data.

\subsubsection{Topic Modeling Visualization (Reviewer Comment \#3)}

To address reviewer feedback requesting a visual representation of topic overlap, we performed Latent Dirichlet Allocation (LDA) topic modeling on the curated dataset. Eight topics were extracted using gensim with automatic hyperparameter optimization. Topic similarity was quantified using Jensen-Shannon divergence between topic-word distributions, and topics were arranged in 2D space using a custom force-directed layout algorithm that positions similar topics closer together. The resulting scatter plot (Supplementary Figure~\ref{SF4}) displays topics as bubbles sized by prevalence, with labels showing the top five terms per topic. This visualization provides an intuitive view of how topics relate to each other and their relative importance in the literature.


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{imgs/SFig1_progression_top30.pdf}
  \caption{Progression of TF-IDF analysis from full corpus to filtered insights.
  (A) Full dataset ($51{,}613$ articles): generic anchors dominate;
  (B) Selected articles ($195$): core themes retained;
  (C) Selected articles + Filtered words: domain-specific trends (e.g., precision medicine, gene expression, human phenotype ontology,
  single cell) become prominent. Horizontal bars show the top-30 phrases per stage.}
  \label{SF1}
\end{figure}


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=\textwidth]{imgs/SFig2_selected_comparison_top30.pdf}
  \caption{Source comparison before filtering generic phrases.
  Grouped bars show TF-IDF scores on the same scale for the union of top-30 phrases across PubMed ($n=131$) and preprints ($n=64$).
  Strong overlap ($57\%$) indicates consensus on core topics before filtering.}
  \label{SF2}
\end{figure}


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.73\textwidth]{imgs/SFig3_selected_filtered_comparison_top30.pdf}
  \caption{Source comparison of research trends after article selection and filtering of generic AI/ML phrases.
  Grouped bars show actual TF-IDF scores for the union of top-30 phrases from PubMed ($n=131$) and preprints ($n=64$) on the same scale.
  Overlap drops to $23\%$, revealing distinct emphases: PubMed emphasizes clinical/translational terms while preprints highlight computational methods.}
  \label{SF3}
\end{figure}


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{imgs/SFig4_topic_scatter.pdf}
  \caption{Topic modeling visualization showing overlap and relationships between themes.
  Eight LDA topics fitted on the curated dataset ($n=195$) are displayed in 2D space using Jensen-Shannon divergence-based layout.
  Bubble size reflects topic prevalence. Labels show top five terms per topic. Topics closer together share more vocabulary,
  illustrating the semantic landscape of LLM applications in medical genomics.}
  \label{SF4}
\end{figure}


\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.73\textwidth]{imgs/SFig5_finetuned_comparison_top30.pdf}
  \caption{Source comparison using fine-tuned analysis (context preserved, post-hoc reweighting).
  This analysis addresses Reviewer Comment \#2 by first training TF-IDF with full context, then down-weighting generic AI/ML terms.
  Results confirm that domain-specific trends remain stable ($23\%$ overlap between sources), validating the filtered analysis approach.
  The fine-tuned method preserves semantic relationships while surfacing specific research emphases.}
  \label{SF5}
\end{figure}
