{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "from Bio import Entrez\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Configure the logging format and level\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e944c4",
   "metadata": {},
   "source": [
    "# Fetching logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f51c48",
   "metadata": {},
   "source": [
    "### Fetch from ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d639b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Entrez for NCBI API\n",
    "Entrez.email = \"anton@gmail.com\"  # Replace with your email\n",
    "\n",
    "# Function to fetch NCBI (PubMed) papers\n",
    "def fetch_ncbi_papers(query):\n",
    "    logging.info('ncbi searching...')\n",
    "    handle = Entrez.esearch(db=\"pubmed\", \n",
    "                            term=query, \n",
    "                            retmax=1000000, \n",
    "                            mindate=\"2023\", \n",
    "                            maxdate=\"2024\")\n",
    "    record = Entrez.read(handle)\n",
    "    id_list = record[\"IdList\"]\n",
    "    handle.close()\n",
    "\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_list), retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    results = []\n",
    "    for article in tqdm(records['PubmedArticle']):\n",
    "        title = article['MedlineCitation']['Article']['ArticleTitle']\n",
    "        abstract = \"\"\n",
    "        try:\n",
    "            abstract = \" \".join(article['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        # Extract publication date\n",
    "        published = \"\"\n",
    "        try:\n",
    "            pub_date = article['MedlineCitation']['Article']['ArticleDate'][0]\n",
    "            published = f\"{pub_date['Year']}-{pub_date['Month']}-{pub_date['Day']}\" if pub_date else \"Unknown\"\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        # Extract DOI (to construct the URL)\n",
    "        doi = None\n",
    "        for id_tag in article['PubmedData']['ArticleIdList']:\n",
    "            if id_tag.attributes['IdType'] == 'doi':\n",
    "                doi = str(id_tag)\n",
    "                break\n",
    "        url = f\"https://doi.org/{doi}\" if doi else \"No DOI\"\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'abstract': abstract,\n",
    "            'url': url,\n",
    "            'published': published,\n",
    "            'source': 'PubMed'\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775922e",
   "metadata": {},
   "source": [
    "### Fetch from medrxiv and biorxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch bioRxiv and medRxiv papers\n",
    "# Function to fetch papers from bioRxiv or medRxiv\n",
    "def fetch_papers_rxiv(server, \n",
    "                      start_date, \n",
    "                      end_date, \n",
    "                      query_terms_list):\n",
    "    results = []\n",
    "    cursor = 0\n",
    "    total_papers = None\n",
    "    new_papers_count = None\n",
    "    while True:\n",
    "        # Construct the API URL with the correct format, iterating over pages using the cursor\n",
    "        api_url = f\"https://api.biorxiv.org/details/{server}/{start_date}/{end_date}/{cursor}/json\"\n",
    "        \n",
    "        # Set headers to mimic a legitimate browser request\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        logging.info(f\"server: {server}; cursor = {cursor}, total = {total_papers}, new = {new_papers_count}, added = {len(results)}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            messages = data.get('messages', [{}])[0]  # Get the first message\n",
    "\n",
    "            # Extract pagination metadata\n",
    "            total_papers = messages.get('total', None)  # Total papers in the database\n",
    "            new_papers_count = messages.get('count_new_papers', None)  # New papers in the range\n",
    "            \n",
    "            papers = data.get('collection', [])\n",
    "            \n",
    "            # Filter the papers based on the provided query terms\n",
    "            for paper in papers:\n",
    "                title = paper['title']\n",
    "                abstract = paper['abstract']\n",
    "                published_date = paper['date']\n",
    "                doi = paper['doi']\n",
    "                url = f\"https://doi.org/{doi}\"\n",
    "                authors = paper['authors']\n",
    "                flag = True\n",
    "                for query_terms in query_terms_list: \n",
    "                # Check if any query term matches the title or abstract\n",
    "                    if not any(term.lower() in title.lower() or term.lower() in abstract.lower() for term in query_terms):\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    results.append({\n",
    "                        'title': title,\n",
    "                        'abstract': abstract,\n",
    "                        'url': url,\n",
    "                        'published': published_date,\n",
    "                        'authors': authors,\n",
    "                        'source': server\n",
    "                    })\n",
    "            \n",
    "            # Pagination: Check if we should continue to the next page\n",
    "            if len(papers) < 100:\n",
    "                break  # Stop if there are fewer than 100 papers or no more cursor\n",
    "            # Update the cursor for the next request\n",
    "            cursor += 100  # Increment the cursor by 100 to get the next page\n",
    "        else:\n",
    "            logging.warning(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to loop through bioRxiv and medRxiv\n",
    "def fetch_rxiv_both_servers(start_date, end_date, query_terms_list):\n",
    "    all_results = []\n",
    "\n",
    "    # Loop through both servers: 'biorxiv' and 'medrxiv'\n",
    "    for server in ['biorxiv', 'medrxiv']:\n",
    "        logging.info(f\"Fetching papers from {server}...\")\n",
    "        server_results = fetch_papers_rxiv(server, start_date, end_date, query_terms_list)\n",
    "        all_results.extend(server_results)\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec11fe",
   "metadata": {},
   "source": [
    "### Fetch from arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06183b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch arXiv papers\n",
    "def fetch_arxiv_papers(query):\n",
    "    logging.info('Arxiv searching...')\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=10000,  # arXiv has a limit of 1000 per query\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "    results = []\n",
    "    i = 0\n",
    "    for result in tqdm(search.results()):\n",
    "        if result.published.year in [2023, 2024]:\n",
    "            results.append({\n",
    "                'title': result.title,\n",
    "                'abstract': result.summary,\n",
    "                'url': result.entry_id,\n",
    "                'published': result.published,\n",
    "                'source': 'arXiv'\n",
    "            })\n",
    "        i+=1\n",
    "        if i%100==0:\n",
    "            sleep(20)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main loop to fetch data for each term in the first list\n",
    "def fetch_and_merge_articles(query_terms_1, query_terms_2, dir_path='./data'):\n",
    "    all_results = []\n",
    "    \n",
    "    for term_1 in query_terms_1:\n",
    "        query = create_search_query(term_1, query_terms_2)\n",
    "        logging.info(f\"Searching for: {query}\")\n",
    "        \n",
    "        # Fetch articles for this query\n",
    "        current_results = fetch_arxiv_papers(query)\n",
    "        \n",
    "        # Append the results\n",
    "        all_results.extend(current_results)\n",
    "        \n",
    "        # Save progress after each term\n",
    "        save_collected(all_results, os.path.join(dir_path, f'0_arxiv_{term_1}.csv'))\n",
    "        sleep(30)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def remove_duplicates(articles):\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Drop duplicates based on title, abstract, and url (which should be unique)\n",
    "    df_unique = df.drop_duplicates(subset=['title', 'abstract', 'url'])\n",
    "    \n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5912b",
   "metadata": {},
   "source": [
    "# Load data using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_query(query_terms):\n",
    "    # Helper function to format the query terms, adding quotes around multi-word terms\n",
    "    def format_terms(terms):\n",
    "        return \" OR \".join([f\"\\\"{term}\\\"\" if \" \" in term else term for term in terms])\n",
    "\n",
    "    # Format the two sets of terms with OR between them\n",
    "    query_part_1 = format_terms(query_terms[0])\n",
    "    query_part_2 = format_terms(query_terms[1])\n",
    "\n",
    "    # Combine the two parts with AND between them\n",
    "    final_query = f\"({query_part_1}) AND ({query_part_2})\"\n",
    "    \n",
    "    return final_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f219e7f",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "query_terms_list = [\n",
    "    [\"genomic\", \"genetic\", \"inherited\", \"hereditary\", \"heredity\", \"inheritance\", \"heritability\", \n",
    "     \"NGS\", \"next genome sequencing\", \"phenotype description\", \"variant interpretation\", \"complex trait\",\n",
    "     \"medicine\", \"medical\", \"clinical decision\",  \"diagnosis\", \"diagnostic\", \"clinical\", \"syndrome\"],\n",
    "    [\"LLM\", \"large language model\", \"NLP\", \"natural language processing\",\n",
    "     \"GPT\", \"chatGPT\", \"transformer\", \"BERT\", \"Bidirectional Encoder Representation\", \n",
    "     \"RAG\", \"retrieval-augmented generation\", \"retrieval augmented generation\", \n",
    "     \"generative AI\", \"AI assistant\", \"prompt\", \"chatbot\", \"prompt engineering\", \n",
    "     \"attention mechanism\", \"chain-of-thought\", \"chain of thought\"]\n",
    "]\n",
    "query = create_search_query(query_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26bb88",
   "metadata": {},
   "source": [
    "### Load from ncbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9918e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now actually search\n",
    "logging.info(\"JUST STARTED\")\n",
    "\n",
    "ncbi_papers = fetch_ncbi_papers(query)\n",
    "logging.info(f\"FINISHED ncbi, len: {len(ncbi_papers)}\")\n",
    "save_collected(ncbi_papers, 'data/0_ncbi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfea00",
   "metadata": {},
   "source": [
    "### Load from biorxiv and medrxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now actually search\n",
    "logging.info(\"JUST STARTED\")\n",
    "\n",
    "bio_medrxiv_papers = fetch_rxiv_both_servers(start_date, end_date, query_terms_list)\n",
    "logging.info(f\"FINISHED biomedrxiv, len: {len(bio_medrxiv_papers)}\")\n",
    "save_collected(bio_medrxiv_papers, 'data/0_bio_med.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac567b08",
   "metadata": {},
   "source": [
    "### Load from arxiv\n",
    "\n",
    "Since arxiv have limits of `1000` returned articles per query, sometimes, you need to specify queries (as it's implemented here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting article collection...\")\n",
    "all_arxiv_papers = fetch_and_merge_articles(query_terms_list[0], query_terms_list[1], dir_path='./data')\n",
    "logging.info(f\"Finished fetching papers, total found: {len(all_arxiv_papers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6698549",
   "metadata": {},
   "source": [
    "Specific subsamples were saved with tha mask `0_arxiv_*.csv`, we can save them all like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "unique_papers = remove_duplicates(all_arxiv_papers)\n",
    "logging.info(f\"Total unique papers: {len(unique_papers)}\")\n",
    "\n",
    "# Save the final result\n",
    "save_collected(unique_papers.to_dict('records'), 'data/0_arxiv_unique.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b9d75",
   "metadata": {},
   "source": [
    "Alternatively (if we did fetching several times), we can merge all together afterwards, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arxiv_files(directory_path='./', pattern='0_arxiv_*.csv', output_file='0_arxiv_unique.csv'):\n",
    "\n",
    "\n",
    "    # Get a list of all files that match the pattern\n",
    "    file_list = [file for file in os.listdir(directory_path) if file.startswith('0_arxiv_') and file.endswith('.csv')]\n",
    "\n",
    "    # If no files found, return a message\n",
    "    if not file_list:\n",
    "        return \"No files found matching the pattern.\"\n",
    "\n",
    "    # Read all the CSV files and set the first column as the index\n",
    "    dfs = [pd.read_csv(os.path.join(directory_path, file), index_col=0) for file in file_list]\n",
    "\n",
    "    # Concatenate all the DataFrames into one\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Get initial number of rows\n",
    "    initial_row_count = len(combined_df)\n",
    "    print(f\"Initial number of rows: {initial_row_count}\")\n",
    "\n",
    "    # Drop NaN values and duplicates\n",
    "    combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Get final number of rows after cleaning\n",
    "    final_row_count = len(combined_df)\n",
    "    print(f\"Final number of rows after cleaning: {final_row_count}\")\n",
    "\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    combined_df.to_csv(os.path.join(directory_path, output_file), index=False)\n",
    "\n",
    "    return initial_row_count, final_row_count, output_file\n",
    "\n",
    "# Call the function and return the result\n",
    "process_arxiv_files(directory_path='./data', pattern='0_arxiv_*.csv', output_file='0_arxiv_unique.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
